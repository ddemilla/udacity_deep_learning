{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# q_table = np.zeros([3**9,9])\n",
    "\n",
    "def gen_q_table():\n",
    "    num_states = 3**9\n",
    "    lookup_dict = {}\n",
    "    for one in range(3):\n",
    "        for two in range(3):\n",
    "            for three in range(3):\n",
    "                for four in range(3):\n",
    "                    for five in range(3):\n",
    "                        for six in range(3):\n",
    "                            for seven in range(3):\n",
    "                                for eight in range(3):\n",
    "                                    for nine in range(3):\n",
    "                                        state = str(one) + str(two) + str(three) + str(four) + str(five) + str(six) + str(seven) + str(eight) + str(nine)\n",
    "                                        lookup_dict[state] = [0 for i in range(9)]\n",
    "    return lookup_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_games = 100\n",
    "learning_rate = .1\n",
    "discount_factor = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_available_moves(state):\n",
    "    available_moves = []\n",
    "    for i in range(9):\n",
    "        if state[i] == 0:\n",
    "            available_moves.append(i)\n",
    "            \n",
    "    return available_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0 = Empty\n",
    "# 1 = Player placed X\n",
    "# 2 = Opponent placed O\n",
    "def random_agent(state, player):\n",
    "    available_moves = find_available_moves(state)\n",
    "    move = available_moves[int(random()*len(available_moves))]\n",
    "    next_state = state\n",
    "    next_state[move] = player\n",
    "    \n",
    "    return move, next_state, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def always_block_and_solve_agent(state, player):\n",
    "    next_state = state\n",
    "    if state == [0 for i in range(9)]:\n",
    "        move = 4\n",
    "        next_state[move] = player\n",
    "        \n",
    "        return move, next_state, False\n",
    "    \n",
    "    if player == 1:\n",
    "        opponent = 2\n",
    "    else:\n",
    "        opponent = 1\n",
    "        \n",
    "    # First determine if agent can solve\n",
    "    for a,b,c in [(0,1,2), (3,4,5), (6,7,8),\n",
    "          (0,3,6), (1,4,7), (2,5,8),\n",
    "          (0,4,8), (2,4,6)]:\n",
    "        if state[a] == state[b] == player or state[a] == state[b] == player or state[b] == state[c] == player or state[a] == state[c] == player:\n",
    "            if min(state[a], state[b], state[c]) == 0:\n",
    "                move_idx = [state[a],state[b],state[c]].index(0)\n",
    "                move = [a,b,c][move_idx]\n",
    "                next_state[move] = player\n",
    "\n",
    "                return move, next_state, False\n",
    "            \n",
    "    # If can't solve, determine if agent can block\n",
    "        # First determine if agent can solve\n",
    "    for a,b,c in [(0,1,2), (3,4,5), (6,7,8),\n",
    "          (0,3,6), (1,4,7), (2,5,8),\n",
    "          (0,4,8), (2,4,6)]:\n",
    "        if state[a] == state[b] == opponent or state[a] == state[b] == opponent or state[b] == state[c] == opponent or state[a] == state[c] == opponent:\n",
    "            if min(state[a], state[b], state[c]) == 0:\n",
    "                move_idx = [state[a],state[b],state[c]].index(0)\n",
    "                move = [a,b,c][move_idx]\n",
    "                next_state[move] = player\n",
    "\n",
    "                return move, next_state, False\n",
    "            \n",
    "    # If can't block or win, become random_agent\n",
    "    move, next_state, illegal = random_agent(state, player)\n",
    "            \n",
    "    return move, next_state, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def player_move(state, value_function, player):\n",
    "    illegal = False\n",
    "#     available_moves = []\n",
    "#     for i in range(9):\n",
    "#         if state[i] == 0:\n",
    "#             available_moves.append(i)\n",
    "            \n",
    "#     max_action_vals = [value_function[val] for val in range(len(value_function)) if val in available_moves]\n",
    "#     max_action = max(max_action_vals)\n",
    "#     max_action_idx = max_action_vals.index(max_action)\n",
    "#     move = available_moves[max_action_idx]\n",
    "    \n",
    "    max_action = max(value_function)\n",
    "    max_action_idx = value_function.index(max_action)\n",
    "    move = max_action_idx\n",
    "    \n",
    "    if state[move] != 0:\n",
    "        illegal = True\n",
    "        return move, state, illegal\n",
    "    else:\n",
    "        next_state = state\n",
    "        next_state[move] = player\n",
    "\n",
    "        return move, next_state, illegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_game(state, illegal, player_val):\n",
    "    reward = 0\n",
    "    if illegal:\n",
    "        reward = -10\n",
    "        return True, reward\n",
    "\n",
    "    if player_val == 1:\n",
    "        opponent_val = 2\n",
    "    else:\n",
    "        opponent_val = 1\n",
    "        \n",
    "    for a,b,c in [(0,1,2), (3,4,5), (6,7,8),\n",
    "              (0,3,6), (1,4,7), (2,5,8),\n",
    "              (0,4,8), (2,4,6)]:\n",
    "        if state[a] == state[b] == state[c] == player_val:\n",
    "            reward = 1\n",
    "            return True, reward\n",
    "        if state[a] == state[b] == state[c] == opponent_val:\n",
    "            reward = -2\n",
    "            return True, reward\n",
    "    if min(state) != 0:\n",
    "        reward = 0\n",
    "        return True, reward\n",
    "    \n",
    "    return False, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_game(state):\n",
    "    print('-------')\n",
    "    print('|' + str(state[0]) + '|' + str(state[1]) + '|' + str(state[2]) + '|')\n",
    "    print('|' + str(state[3]) + '|' + str(state[4]) + '|' + str(state[5]) + '|')\n",
    "    print('|' + str(state[6]) + '|' + str(state[7]) + '|' + str(state[8]) + '|')\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(num_games, learning_rate, discount_factor, explore_factor, self_play_prob=.5, q_table=gen_q_table(), print_results=False, agent_name=random_agent):\n",
    "    agent = lambda agent_name, state, player: agent_name(state, player)\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        state = [0 for j in range(9)]\n",
    "        game_over = False\n",
    "        first_move = random() <= .5\n",
    "        play_self = random() <= self_play_prob\n",
    "\n",
    "        if not first_move:\n",
    "            if play_self:\n",
    "                state_id = ''.join([str(i) for i in state])\n",
    "                _, state, illegal = player_move(state, q_table[state_id], 2)\n",
    "            else:\n",
    "                _, state, illegal = agent(agent_name, state, 2)\n",
    "                \n",
    "            if illegal:\n",
    "                break\n",
    "                \n",
    "        while not game_over:\n",
    "            state_id = ''.join([str(j) for j in state])\n",
    "                \n",
    "            # Currently always goes first (fix)\n",
    "            explore = random() <= explore_factor\n",
    "            \n",
    "            if explore:\n",
    "                q_move, q_state, illegal = random_agent(state, 1)\n",
    "            else:\n",
    "                q_move, q_state, illegal = player_move(state, q_table[state_id], 1) \n",
    "            game_over, reward = evaluate_game(q_state, illegal, 1)\n",
    "            if game_over == True:\n",
    "                if print_results:\n",
    "                    if reward == 1:\n",
    "                        print(\"Player wins\")\n",
    "                    else:\n",
    "                        print(\"Cat Game\")\n",
    "                    print_game(q_state)\n",
    "                q_table[state_id][q_move] = reward\n",
    "                \n",
    "                break\n",
    "        \n",
    "            if print_results:\n",
    "                print_game(q_state)\n",
    "                \n",
    "            if play_self:\n",
    "                if print_results:\n",
    "                    print(\"Playing Self\")\n",
    "                q_state_id = ''.join([str(s) for s in q_state])\n",
    "                opponent_move, new_state, illegal = player_move(state, q_table[q_state_id], 2)\n",
    "            else:\n",
    "                if print_results:\n",
    "                    print(\"Playing Opponent\")\n",
    "                opponent_move, new_state, illegal = agent(agent_name, q_state, 2) \n",
    "            game_over, reward = evaluate_game(q_state, illegal, 1)\n",
    "            if game_over == True:\n",
    "                if print_results:\n",
    "                    if reward < 0:\n",
    "                        print(\"Opponent Wins\")\n",
    "                    else:\n",
    "                        print(\"Cat Game\")\n",
    "                    print_game(new_state)\n",
    "                \n",
    "                q_table[state_id][q_move] = reward\n",
    "                \n",
    "                break\n",
    "\n",
    "            # Update q table\n",
    "            new_state_id = ''.join([str(s) for s in new_state])\n",
    "            max_next = max(q_table[new_state_id])\n",
    "            \n",
    "            q_table_update = learning_rate*(discount_factor * max_next - q_table[state_id][q_move])\n",
    "            if print_results:\n",
    "                print(q_table_update)\n",
    "            q_table[state_id][q_move] += q_table_update\n",
    "\n",
    "            if game_over == True:\n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "            if print_results:\n",
    "                print_game(state)\n",
    "\n",
    "        if i % (num_games // 10) == 0:\n",
    "            explore_factor = explore_factor * (num_games - i) / num_games\n",
    "            print(\"Num iteration: %d, Explore Factor: %f\" % (i, explore_factor))\n",
    "            print(\"Number games lost: %d\\nAverage Reward: %f\" % evaluate_against_block_and_solve(q_table,1000))\n",
    "#             print(\"Number games lost: %d\\nAverage Reward: %f\" % evaluate_against_random(q_table,1000))\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_play(q_table, state):\n",
    "    state_id = ''.join([str(i) for i in state])\n",
    "    value_function = q_table[state_id]\n",
    "    available_moves = []\n",
    "    for i in range(9):\n",
    "        if state[i] == 0:\n",
    "            available_moves.append(i)\n",
    "            \n",
    "    max_action_vals = [value_function[val] for val in range(len(value_function)) if val in available_moves]\n",
    "    max_action = max(max_action_vals)\n",
    "    max_action_idx = max_action_vals.index(max_action)\n",
    "    move = available_moves[max_action_idx]\n",
    "    \n",
    "    next_state = state\n",
    "    next_state[move] = 1\n",
    "    \n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_random_agent(q_table, print_results=True):\n",
    "    state = [0 for j in range(9)]\n",
    "    for j in range(6):\n",
    "        state_id = ''.join([str(i) for i in state])\n",
    "\n",
    "        q_move, q_state, illegal = player_move(state, q_table[state_id], 1) \n",
    "        game_over, reward = evaluate_game(q_state, illegal, 1)\n",
    "\n",
    "        if game_over == True:\n",
    "            if print_results:\n",
    "                if reward == 1:\n",
    "                    print(\"Player wins\")\n",
    "                else:\n",
    "                    print(\"Cat Game\")\n",
    "                print_game(q_state)\n",
    "            q_table[state_id][q_move] = reward\n",
    "\n",
    "            return reward\n",
    "\n",
    "        random_move, new_state, illegal = random_agent(q_state, 2) \n",
    "        game_over, reward = evaluate_game(q_state, illegal, 1)\n",
    "        if game_over == True:\n",
    "            if print_results:\n",
    "                if reward < 0:\n",
    "                    print(\"Opponent Wins\")\n",
    "                else:\n",
    "                    print(\"Cat Game\")\n",
    "                print_game(new_state)\n",
    "\n",
    "            q_table[state_id][q_move] = reward\n",
    "\n",
    "            return reward\n",
    "        \n",
    "        state = new_state\n",
    "        if print_results:\n",
    "            print_game(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_block_and_solve(q_table, print_results=True):\n",
    "    state = [0 for j in range(9)]\n",
    "    first_move = random() <= .5\n",
    "\n",
    "    if not first_move:\n",
    "        if print_results:\n",
    "            print(\"Opponent first move\")\n",
    "            \n",
    "        _, state, illegal = always_block_and_solve_agent(state, 2)\n",
    "        \n",
    "    else:\n",
    "        if print_results:\n",
    "            print(\"Player first\")\n",
    "                \n",
    "    for j in range(6):\n",
    "        state_id = ''.join([str(i) for i in state])\n",
    "\n",
    "        q_move, q_state, illegal = player_move(state, q_table[state_id], 1) \n",
    "        game_over, reward = evaluate_game(q_state, illegal, 1)\n",
    "        if game_over == True:\n",
    "            if print_results:\n",
    "                if reward == 1:\n",
    "                    print(\"Player wins\")\n",
    "                else:\n",
    "                    print(\"Cat Game\")\n",
    "                print_game(q_state)\n",
    "            q_table[state_id][q_move] = reward\n",
    "\n",
    "            return reward\n",
    "        \n",
    "        if print_results:\n",
    "            print_game(q_state)\n",
    "\n",
    "        random_move, new_state, illegal = always_block_and_solve_agent(q_state, 2) \n",
    "        game_over, reward = evaluate_game(q_state, illegal,1)\n",
    "        if game_over == True:\n",
    "            if print_results:\n",
    "                if reward < 0:\n",
    "                    print(\"Opponent Wins\")\n",
    "                else:\n",
    "                    print(\"Cat Game\")\n",
    "                print_game(new_state)\n",
    "\n",
    "            q_table[state_id][q_move] = reward\n",
    "\n",
    "            return reward\n",
    "        \n",
    "        state = new_state\n",
    "        if print_results:\n",
    "            print_game(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_against_random(q_table, num_loops):\n",
    "    num_lost = 0\n",
    "    average_reward = 0\n",
    "    for _ in range(num_loops):\n",
    "        reward = play_random_agent(q_table,print_results=False)\n",
    "        if reward < 0:\n",
    "            num_lost += 1\n",
    "        average_reward += reward / num_loops\n",
    "        \n",
    "    return num_lost, average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_against_block_and_solve(q_table, num_loops):\n",
    "    num_lost = 0\n",
    "    average_reward = 0\n",
    "    for _ in range(num_loops):\n",
    "        reward = play_block_and_solve(q_table,print_results=False)\n",
    "        if reward < 0:\n",
    "            num_lost += 1\n",
    "        average_reward += reward / num_loops\n",
    "        \n",
    "    return num_lost, average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_player():\n",
    "    q_table = train(200000, .1, .2, 1,self_play_prob=0,agent_name=random_agent)\n",
    "    q_table = train(200000, .1, .99, 1,self_play_prob=0,q_table=q_table,agent_name=always_block_and_solve_agent)\n",
    "    q_table = train(400000, .1, .2, .5,self_play_prob=0,q_table=q_table,agent_name=random_agent)\n",
    "    q_table = train(400000, .1, .99, .5,self_play_prob=0,q_table=q_table,agent_name=always_block_and_solve_agent)\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "move_freqs = {key: 0 for key in range(2)}\n",
    "num_iters = 100000\n",
    "for i in range(num_iters):\n",
    "    move = int(random()*2)\n",
    "    move_freqs[move] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 49989, 1: 50011}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [0 for i in range(9)]\n",
    "print(''.join([str(i) for i in test]))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = [1, 2, 1, 0, 0, 0, 1, 1, 1]\n",
    "available_moves = []\n",
    "for i in range(9):\n",
    "    if state[i] == 0:\n",
    "        available_moves.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, -2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_game([2, 2, 1, 2, 0, 0, 2, 0, 0], False, 1)\n",
    "# print_game([2, 2, 1, 2, 0, 0, 2, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num iteration: 0, Explore Factor: 1.000000\n",
      "Number games lost: 111\n",
      "Average Reward: -0.078000\n",
      "Num iteration: 40000, Explore Factor: 0.900000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.420000\n",
      "Num iteration: 80000, Explore Factor: 0.720000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.477000\n",
      "Num iteration: 120000, Explore Factor: 0.504000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.510000\n",
      "Num iteration: 160000, Explore Factor: 0.302400\n",
      "Number games lost: 0\n",
      "Average Reward: 0.503000\n",
      "Num iteration: 200000, Explore Factor: 0.151200\n",
      "Number games lost: 0\n",
      "Average Reward: 0.493000\n",
      "Num iteration: 240000, Explore Factor: 0.060480\n",
      "Number games lost: 0\n",
      "Average Reward: 0.498000\n",
      "Num iteration: 280000, Explore Factor: 0.018144\n",
      "Number games lost: 0\n",
      "Average Reward: 0.503000\n",
      "Num iteration: 320000, Explore Factor: 0.003629\n",
      "Number games lost: 0\n",
      "Average Reward: 0.470000\n",
      "Num iteration: 360000, Explore Factor: 0.000363\n",
      "Number games lost: 0\n",
      "Average Reward: 0.489000\n",
      "CPU times: user 31.2 s, sys: 77.4 ms, total: 31.2 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# q_table = train(1, .1, .9, 0,q_table,True)\n",
    "q_table = train(400000, .1, .99, 1,self_play_prob=0,agent_name=always_block_and_solve_agent)\n",
    "# q_table = train(10000, .1, .2, 1,self_play_prob=1,q_table=q_table)\n",
    "# q_table = train(500000, .2, .3, 1, q_table=q_table, agent_name=random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num iteration: 0, Explore Factor: 1.000000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.470000\n",
      "Num iteration: 20000, Explore Factor: 0.900000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.462000\n",
      "Num iteration: 40000, Explore Factor: 0.720000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.472000\n",
      "Num iteration: 60000, Explore Factor: 0.504000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.285000\n",
      "Num iteration: 80000, Explore Factor: 0.302400\n",
      "Number games lost: 0\n",
      "Average Reward: 0.390000\n",
      "Num iteration: 100000, Explore Factor: 0.151200\n",
      "Number games lost: 0\n",
      "Average Reward: 0.476000\n",
      "Num iteration: 120000, Explore Factor: 0.060480\n",
      "Number games lost: 0\n",
      "Average Reward: 0.224000\n",
      "Num iteration: 140000, Explore Factor: 0.018144\n",
      "Number games lost: 0\n",
      "Average Reward: 0.255000\n",
      "Num iteration: 160000, Explore Factor: 0.003629\n",
      "Number games lost: 0\n",
      "Average Reward: 0.378000\n",
      "Num iteration: 180000, Explore Factor: 0.000363\n",
      "Number games lost: 0\n",
      "Average Reward: 0.276000\n",
      "Num iteration: 0, Explore Factor: 1.000000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.215000\n",
      "Num iteration: 20000, Explore Factor: 0.900000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.506000\n",
      "Num iteration: 40000, Explore Factor: 0.720000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.493000\n",
      "Num iteration: 60000, Explore Factor: 0.504000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.494000\n",
      "Num iteration: 80000, Explore Factor: 0.302400\n",
      "Number games lost: 0\n",
      "Average Reward: 0.503000\n",
      "Num iteration: 100000, Explore Factor: 0.151200\n",
      "Number games lost: 0\n",
      "Average Reward: 0.488000\n",
      "Num iteration: 120000, Explore Factor: 0.060480\n",
      "Number games lost: 0\n",
      "Average Reward: 0.472000\n",
      "Num iteration: 140000, Explore Factor: 0.018144\n",
      "Number games lost: 0\n",
      "Average Reward: 0.493000\n",
      "Num iteration: 160000, Explore Factor: 0.003629\n",
      "Number games lost: 0\n",
      "Average Reward: 0.475000\n",
      "Num iteration: 180000, Explore Factor: 0.000363\n",
      "Number games lost: 0\n",
      "Average Reward: 0.489000\n",
      "Num iteration: 0, Explore Factor: 0.500000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.521000\n",
      "Num iteration: 40000, Explore Factor: 0.450000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.278000\n",
      "Num iteration: 80000, Explore Factor: 0.360000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.303000\n",
      "Num iteration: 120000, Explore Factor: 0.252000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.200000\n",
      "Num iteration: 160000, Explore Factor: 0.151200\n",
      "Number games lost: 0\n",
      "Average Reward: 0.285000\n",
      "Num iteration: 200000, Explore Factor: 0.075600\n",
      "Number games lost: 0\n",
      "Average Reward: 0.406000\n",
      "Num iteration: 240000, Explore Factor: 0.030240\n",
      "Number games lost: 25\n",
      "Average Reward: 0.223000\n",
      "Num iteration: 280000, Explore Factor: 0.009072\n",
      "Number games lost: 0\n",
      "Average Reward: 0.360000\n",
      "Num iteration: 320000, Explore Factor: 0.001814\n",
      "Number games lost: 68\n",
      "Average Reward: 0.018000\n",
      "Num iteration: 360000, Explore Factor: 0.000181\n",
      "Number games lost: 124\n",
      "Average Reward: -0.079000\n",
      "Num iteration: 0, Explore Factor: 0.500000\n",
      "Number games lost: 63\n",
      "Average Reward: 0.148000\n",
      "Num iteration: 40000, Explore Factor: 0.450000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.480000\n",
      "Num iteration: 80000, Explore Factor: 0.360000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.474000\n",
      "Num iteration: 120000, Explore Factor: 0.252000\n",
      "Number games lost: 0\n",
      "Average Reward: 0.482000\n",
      "Num iteration: 160000, Explore Factor: 0.151200\n",
      "Number games lost: 0\n",
      "Average Reward: 0.532000\n",
      "Num iteration: 200000, Explore Factor: 0.075600\n",
      "Number games lost: 0\n",
      "Average Reward: 0.517000\n",
      "Num iteration: 240000, Explore Factor: 0.030240\n",
      "Number games lost: 0\n",
      "Average Reward: 0.497000\n",
      "Num iteration: 280000, Explore Factor: 0.009072\n",
      "Number games lost: 0\n",
      "Average Reward: 0.483000\n",
      "Num iteration: 320000, Explore Factor: 0.001814\n",
      "Number games lost: 0\n",
      "Average Reward: 0.487000\n",
      "Num iteration: 360000, Explore Factor: 0.000181\n",
      "Number games lost: 0\n",
      "Average Reward: 0.476000\n",
      "CPU times: user 1min 16s, sys: 109 ms, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_table = train_player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ca541a8bb919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# new_state = q_play(q_table, state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malways_block_and_solve_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "state = [0 for i in range(9)]\n",
    "# new_state = q_play(q_table, state)\n",
    "_, new_state = always_block_and_solve_agent(state, 1)\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-af3026300a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# new_state = q_play(q_table, state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_state' is not defined"
     ]
    }
   ],
   "source": [
    "state = [1, 1, 0, 1, 2, 0, 0, 0, 2]\n",
    "print(convert_state(state))\n",
    "print_game(state)\n",
    "state[6] = 2\n",
    "# new_state = q_play(q_table, state)\n",
    "_, new_state = always_block_and_solve_agent(state, 1)\n",
    "print(new_state)\n",
    "print_game(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_game() takes exactly 3 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-be6123e78479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_game() takes exactly 3 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "evaluate_game([1, 2, 1, 1, 2, 0, 0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200221020'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(q_table.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6585606013976042,\n",
       " 0.6910349083309222,\n",
       " 0.6443684584124908,\n",
       " 0.6573302579538166,\n",
       " 0.6850609599488371,\n",
       " 0.6854595703193759,\n",
       " 0.9555289349572427,\n",
       " 0.6742865201224162,\n",
       " 0.6565291132415879]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table['000000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table['000000000'].index(max(q_table['000000000']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "|0|0|0|\n",
      "|0|0|2|\n",
      "|0|1|0|\n",
      "-------\n",
      "-------\n",
      "|0|0|0|\n",
      "|0|0|2|\n",
      "|1|1|2|\n",
      "-------\n",
      "-------\n",
      "|0|0|1|\n",
      "|0|2|2|\n",
      "|1|1|2|\n",
      "-------\n",
      "-------\n",
      "|1|2|1|\n",
      "|0|2|2|\n",
      "|1|1|2|\n",
      "-------\n",
      "Player wins\n",
      "-------\n",
      "|1|2|1|\n",
      "|1|2|2|\n",
      "|1|1|2|\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "reward = play_random_agent(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, -4.050000000000011)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_against_random(q_table,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player first\n",
      "-------\n",
      "|0|0|0|\n",
      "|0|0|0|\n",
      "|0|0|1|\n",
      "-------\n",
      "-------\n",
      "|2|0|0|\n",
      "|0|0|0|\n",
      "|0|0|1|\n",
      "-------\n",
      "-------\n",
      "|2|0|1|\n",
      "|0|0|0|\n",
      "|0|0|1|\n",
      "-------\n",
      "-------\n",
      "|2|0|1|\n",
      "|0|0|2|\n",
      "|0|0|1|\n",
      "-------\n",
      "-------\n",
      "|2|0|1|\n",
      "|0|0|2|\n",
      "|1|0|1|\n",
      "-------\n",
      "-------\n",
      "|2|0|1|\n",
      "|0|0|2|\n",
      "|1|2|1|\n",
      "-------\n",
      "Player wins\n",
      "-------\n",
      "|2|0|1|\n",
      "|0|1|2|\n",
      "|1|2|1|\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_block_and_solve(q_table, print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent lost: 0.00543, Average Reward: 0.97\n"
     ]
    }
   ],
   "source": [
    "num_games = 100000\n",
    "# num_lost, reward = evaluate_against_block_and_solve(q_table,num_games)\n",
    "num_lost, reward = evaluate_against_random(q_table,num_games)\n",
    "print(\"Percent lost: %.5f, Average Reward: %.2f\" % (num_lost / num_games, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=18, \n",
    "                 action_size=9, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This lext line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # expotentional decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    new_state = [0 for i in range(18)]\n",
    "    \n",
    "    for s in range(len(state)):\n",
    "        if state[s] != 0:\n",
    "            new_state_idx = int(9 * (state[s] - 1) + s)\n",
    "            new_state[new_state_idx] = 1\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_episodes = 100000          # max number of episodes to learn from\n",
    "gamma = 0.9                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # expotentional decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.01         # Q-network learning rate\n",
    "player_val = 1\n",
    "agent_name = always_block_and_solve_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step_game(state, action, player_val, agent=random_agent):\n",
    "    if state[action] != 0:\n",
    "        illegal = True\n",
    "    else:\n",
    "        illegal = False\n",
    "    \n",
    "    if player_val == 1:\n",
    "        opponent_val = 2\n",
    "    else:\n",
    "        opponent_val = 1\n",
    "        \n",
    "    q_state = state\n",
    "    q_state[action] = player_val\n",
    "    \n",
    "    game_over, reward = evaluate_game(state, illegal, player_val)\n",
    "    \n",
    "    if game_over:\n",
    "        return state, game_over, reward\n",
    "    \n",
    "    _, new_state, illegal = agent_name(state, opponent_val)\n",
    "    \n",
    "    game_over, reward = evaluate_game(state, illegal, player_val)\n",
    "    \n",
    "    return new_state, game_over, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_game(player_val, agent=random_agent):\n",
    "    state = [0 for s in range(9)]\n",
    "    if np.random.rand() <= .5:\n",
    "        return state\n",
    "    else:\n",
    "        if player_val == 1:\n",
    "            opponent_val = 2\n",
    "        else:\n",
    "            opponent_val = 1\n",
    "            \n",
    "        _, state, _ = agent(state, opponent_val)\n",
    "    \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000 Average reward: -1.625062506250625 Explore P: 0.9058\n",
      "Average Loss: 0.26\n",
      "Episode: 20000 Average reward: -1.6166 Explore P: 0.8205\n",
      "Average Loss: 0.08\n",
      "Episode: 30000 Average reward: -1.6253 Explore P: 0.7434\n",
      "Average Loss: 0.07\n",
      "Episode: 40000 Average reward: -1.6601 Explore P: 0.6736\n",
      "Average Loss: 0.27\n",
      "Episode: 50000 Average reward: -1.665 Explore P: 0.6105\n",
      "Average Loss: 0.04\n",
      "Episode: 60000 Average reward: -1.6754 Explore P: 0.5533\n",
      "Average Loss: 0.15\n",
      "Episode: 70000 Average reward: -1.6846 Explore P: 0.5016\n",
      "Average Loss: 0.22\n",
      "Episode: 80000 Average reward: -1.6998 Explore P: 0.4548\n",
      "Average Loss: 0.15\n",
      "Episode: 90000 Average reward: -1.712 Explore P: 0.4125\n",
      "Average Loss: 0.07\n",
      "-1.6673299999976314\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "loss_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    average_reward = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        state = initialize_game(player_val, agent=agent_name)\n",
    "#         print(\"Initial State\")\n",
    "#         print(state)\n",
    "        done = False\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        while not done:\n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*(ep/10)) \n",
    "#             explore_p = 0\n",
    "            if explore_p > np.random.rand():\n",
    "#                 # Make a random action\n",
    "#                 print(\"Random action\")\n",
    "                available_moves = find_available_moves(state)\n",
    "                action = available_moves[int(random()*len(available_moves))]\n",
    "#                 print(\"After random action\")\n",
    "#                 print(state)\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                \n",
    "#                 q_move, q_state, illegal = player_move(state, q_table[state_id], 1) \n",
    "                converted_state = np.asarray(convert_state(state))\n",
    "                feed = {mainQN.inputs_: converted_state.reshape(1,*converted_state.shape)}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "#                 print(\"Qs\")\n",
    "#                 print(Qs)\n",
    "                available_moves = find_available_moves(state)\n",
    "    \n",
    "                max_action_vals = [state[val] for val in range(len(state)) if val in available_moves]\n",
    "                max_action = max(max_action_vals)\n",
    "                max_action_idx = max_action_vals.index(max_action)\n",
    "                action = available_moves[max_action_idx]\n",
    "#                 print(\"Available_moves\")\n",
    "#                 print(available_moves)\n",
    "#                 move_vals = [s for s in state if s in available_moves]\n",
    "#                 print(\"State\")\n",
    "#                 print(state)\n",
    "#                 print(\"Move Vals\")\n",
    "#                 print(move_vals)\n",
    "#                 action = np.argmax(move_vals)\n",
    "#                 print(action)\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            \n",
    "#             print(\"state\")\n",
    "#             print_game(state)\n",
    "#             print(\"action\")\n",
    "#             print(action)\n",
    "            next_state, done, reward = step_game(state, action, player_val, agent=agent_name)\n",
    "#             print(\"Next state\")\n",
    "#             print_game(next_state)\n",
    "            total_reward += reward\n",
    "            average_reward += reward/train_episodes\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "#                 next_state = np.zeros(9)\n",
    "#                 next_state = [-1 for z in range(9)]\n",
    "                \n",
    "                if ep % (train_episodes // 10) == 0:\n",
    "                    print('Episode: {}'.format(ep),\n",
    "                          'Average reward: {}'.format(np.mean(rewards_list)),\n",
    "#                           'Training loss: {:.4f}'.format(loss),\n",
    "                          'Explore P: {:.4f}'.format(explore_p))\n",
    "                    rewards_list = []\n",
    "                    \n",
    "                rewards_list.append(reward)\n",
    "\n",
    "            else:\n",
    "                state = next_state\n",
    "            next_states.append(convert_state(next_state))\n",
    "            states.append(convert_state(state))\n",
    "            rewards.append(reward)\n",
    "            \n",
    "#             # Sample mini-batch from memory\n",
    "#             batch = memory.sample(batch_size)\n",
    "#             states = np.array([each[0] for each in batch])\n",
    "#             actions = np.array([each[1] for each in batch])\n",
    "#             rewards = np.array([each[2] for each in batch])\n",
    "#             next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "#             # Train network\n",
    "        target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "\n",
    "#             episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "#         print(\"Target Qs\")\n",
    "#         print(target_Qs)\n",
    "#             target_Qs[episode_ends] = (0, 0)\n",
    "#         print(\"states\")\n",
    "#         print(states)\n",
    "#         print(\"Outputs\")\n",
    "#         print(Qs)\n",
    "#         print(\"Rewards\")\n",
    "#         print(rewards)\n",
    "        targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "#         print(\"Targets\")\n",
    "#         print(targets)\n",
    "#         print(\"Actions\")\n",
    "#         print(actions)\n",
    "        loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                            feed_dict={mainQN.inputs_: states,\n",
    "                                       mainQN.targetQs_: targets,\n",
    "                                       mainQN.actions_: actions})\n",
    "    \n",
    "        loss_list.append(loss)\n",
    "        if ep % (train_episodes // 10) == 0:\n",
    "            print(\"Average Loss: %.2f\" % np.mean(loss_list))\n",
    "            loss_list = []\n",
    "    print(average_reward)\n",
    "    saver.save(sess, \"checkpoints/tic-tac-toe.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "|0|0|0|\n",
      "|0|0|0|\n",
      "|0|0|0|\n",
      "-------\n",
      "[[ 1.17139781  1.21745515  0.96601009  1.22202933  1.25242293  1.27167892\n",
      "   1.25516498  1.24944532  1.15785098]]\n",
      "5\n",
      "-------\n",
      "|0|0|0|\n",
      "|0|0|1|\n",
      "|0|0|2|\n",
      "-------\n",
      "[[ 0.77960378  0.7806859   0.59929162  0.75685883  0.76190478  0.7611509\n",
      "   0.79187161  0.80504137  0.7632966 ]]\n",
      "7\n",
      "-------\n",
      "|0|0|0|\n",
      "|0|2|1|\n",
      "|0|1|2|\n",
      "-------\n",
      "[[ 0.90489191  0.94092888  0.74684429  0.91692197  0.96818149  0.94752759\n",
      "   0.96581078  0.96296114  0.88441658]]\n",
      "4\n",
      "-10\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"checkpoints/tic-tac-toe.ckpt\")\n",
    "    \n",
    "    for ep in range(1, test_episodes):\n",
    "        game_over = False\n",
    "        state = initialize_game(player_val, agent=agent_name)\n",
    "        \n",
    "        while not game_over:\n",
    "            print_game(state)\n",
    "\n",
    "            \n",
    "            # Get action from Q-network\n",
    "            converted_state = np.asarray(convert_state(state))\n",
    "            feed = {mainQN.inputs_: converted_state.reshape(1,*converted_state.shape)}\n",
    "            Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "            action = np.argmax(Qs)\n",
    "            next_state, game_over, reward = step_game(state, action, player_val, agent=agent_name)\n",
    "            print(Qs)\n",
    "            print(action)\n",
    "            if game_over:\n",
    "                print(reward)\n",
    "\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.zeros(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
